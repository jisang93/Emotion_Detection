{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Facial_Expression_Detection.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOBFVcpxxe9QnraTVhidJug"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"iYmAvlW91cr1","colab_type":"code","colab":{}},"source":["from mpl_toolkits.mplot3d import Axes3D\n","from sklearn.preprocessing import StandardScaler\n","from google.colab.patches import cv2_imshow\n","from tensorflow.keras import datasets, layers, models\n","from tensorflow.keras.applications import imagenet_utils\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.applications import VGG19\n","\n","import dlib, cv2, os\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.patheffects as path_effects\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PPdI6Ti-2Q4T","colab_type":"text"},"source":["# **표정 분석 모델링**"]},{"cell_type":"markdown","metadata":{},"source":["## **1. 데이터 불러오기**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 이미지 데이터 로드\n","import pickle\n","import gzip\n","\n","with gzip.open('C:/Users/jisan/workspace/표정 감정 분석/Emotion_Detection/Data_Preprocessing/dataset',\"rb\") as fr:\n","    img_result = pickle.load(fr)\n","with gzip.open('C:/Users/jisan/workspace/표정 감정 분석/Emotion_Detection/Data_Preprocessing/dataset',\"rb\") as fr:\n","    emotion_result = pickle.load(fr)\n","with gzip.open('C:/Users/jisan/workspace/표정 감정 분석/Emotion_Detection/Data_Preprocessing/dataset',\"rb\") as fr:\n","    face_result = pickle.load(fr)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 이미지와 감정 라벨의 길이가 같은 것을 확인\n","len(img_result), len(emotion_result), len(face_result)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Face_result 결과 확인\n","fig, axes = plt.subplots(1, 2, figsize=(16,10))\n","axes[0].imshow(cv2.cvtColor(img_result[100], cv2.COLOR_BGR2RGB))\n","axes[1].imshow(cv2.cvtColor(face_result[100], cv2.COLOR_BGR2RGB))"]},{"cell_type":"markdown","metadata":{"id":"6vOheFWA2SoF","colab_type":"text"},"source":["## **2. 데이터 행렬 처리**"]},{"cell_type":"code","metadata":{"id":"I1LXUP6Q2O6p","colab_type":"code","colab":{}},"source":["# 데이터 list reshape\n","image_set = np.asarray(face_result)\n","emotion_name = np.asarray(emotion_result)\n","# image_set(x값)과 emotion_name(y값)의 shape 확인\n","image_set.shape, emotion_name.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uo_SKpVl2Unc","colab_type":"code","colab":{}},"source":["# Train_set, Test_set 만들기\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# x값 형태 맞추기\n","X = image_set[:, :, :, :]\n","\n","# String 형태의 y값을 LabelEncoder로 변환\n","Y_obj = emotion_name[:]\n","e = LabelEncoder()\n","e.fit(Y_obj)\n","Y = e.transform(Y_obj)\n","\n","# Train, Tes set형성 / stratify를 통해 y 기준으로 셔플\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)\n","\n","# Y_train과 Y_test의 형태 변경\n","Y_train = Y_train.reshape(-1, 1)\n","Y_test = Y_test.reshape(-1,1)\n","\n","# X_train, Y_train 형태 확인\n","X_train.shape, Y_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yVltdD_a2Ye7","colab_type":"text"},"source":["## **3. 모델링**"]},{"cell_type":"code","metadata":{"id":"caF_43vl7Bl0","colab_type":"code","colab":{}},"source":["# CNN\n","model = models.Sequential()\n","model.add(layers.Conv2D(64, (3, 3), activation='relu', strides=(2,2), padding='SAME', input_shape=(46, 46, 3)))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","#model.add(layers.Dropout(0.5))\n","\n","model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='SAME', strides=(2,2)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Dropout(0.5))\n","\n","model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='SAME', strides=(2,2)))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.Dropout(0.5))\n","\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='SAME', strides=(2,2)))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.Dropout(0.5))\n","\n","model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='SAME', strides=(2,2)))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.Dropout(0.5))\n","\n","model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='SAME', strides=(2,2)))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.Dropout(0.5))\n","\n","model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(7, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TeRoOBHy2ZqC","colab_type":"code","colab":{}},"source":["# 16층 VGG넷 모델\n","model = models.Sequential()\n","model.add(layers.Conv2D(64, (3, 3), activation='relu',input_shape=(256, 256, 3)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","\n","model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","\n","model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","\n","model.add(layers.Flatten())\n","model.add(layers.Dense(256, activation='relu'))\n","model.add(layers.Dense(128, activation='relu'))\n","model.add(layers.Dense(7, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5EDkh56p2ay6","colab_type":"code","colab":{}},"source":["# 13층 VGG넷 모델\n","model = models.Sequential()\n","model.add(layers.Conv2D(64, (3, 3), activation='relu',input_shape=(256, 256, 3)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.Dropout(0.5))\n","\n","model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.Dropout(0.5))\n","\n","model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.Dropout(0.5))\n","\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.Dropout(0.5))\n","\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.Dropout(0.5))\n","\n","model.add(layers.Flatten())\n","model.add(layers.Dense(256, activation='relu'))\n","model.add(layers.Dense(128, activation='relu'))\n","model.add(layers.Dense(7, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nBvA8l1a2cAm","colab_type":"code","colab":{}},"source":["# ALEX넷 모델\n","model = models.Sequential()\n","\n","# 1번째 Convolution Layer\n","model.add(layers.Conv2D(64, (3, 3), activation='relu',input_shape=(256, 256, 3)))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.BatchNormalization())\n","model.add(layers.Dropout(0.8))\n","\n","# 2번째 Convolution Layer\n","model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.BatchNormalization())\n","model.add(layers.Dropout(0.8))\n","\n","# 3번째 Convolution Layer\n","model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.BatchNormalization())\n","model.add(layers.Dropout(0.8))\n","\n","# 4번째 Convolution Layer\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.BatchNormalization())\n","model.add(layers.Dropout(0.8))\n","\n","# 5번째 Convolution Layer\n","model.add(layers.Conv2D(1024, (3, 3), activation='relu', padding='SAME'))\n","model.add(layers.MaxPooling2D((2, 2), padding='SAME'))\n","model.add(layers.BatchNormalization())\n","model.add(layers.Dropout(0.8))\n","\n","# Fully Connected Layer\n","model.add(layers.Flatten())\n","model.add(layers.Dense(128, activation='relu'))\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(7, activation='softmax'))# 모델 컴파일\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"no4jH4cS_mij","colab_type":"code","colab":{}},"source":["# Resnet50\n","from keras import models, layers\n","from keras import Input\n","from keras.models import Model, load_model\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras import optimizers, initializers, regularizers, metrics\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras.layers import BatchNormalization, Conv2D, Activation, Dense, GlobalAveragePooling2D, MaxPooling2D, ZeroPadding2D, Add\n"," \n","# number of classes\n","K = 7\n","\n","input_tensor = Input(shape=(256, 256, 3), dtype='float32', name='input')\n"," \n","def conv1_layer(x):    \n","    x = ZeroPadding2D(padding=(3, 3))(x)\n","    x = Conv2D(64, (7, 7), strides=(2, 2))(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = ZeroPadding2D(padding=(1,1))(x)\n","    return x  \n"," \n","def conv2_layer(x):         \n","    x = MaxPooling2D((3, 3), 2)(x)     \n","    shortcut = x\n","\n","    for i in range(3):\n","        if (i == 0):\n","            x = Conv2D(64, (1, 1), strides=(1, 1), padding='valid')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n","\n","            x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n"," \n","            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\n","            shortcut = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(shortcut)            \n","            x = BatchNormalization()(x)\n","            shortcut = BatchNormalization()(shortcut)\n"," \n","            x = Add()([x, shortcut])\n","            x = Activation('relu')(x)\n","            shortcut = x\n"," \n","        else:\n","            x = Conv2D(64, (1, 1), strides=(1, 1), padding='valid')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n","            \n","            x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n"," \n","            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\n","            x = BatchNormalization()(x)            \n"," \n","            x = Add()([x, shortcut])   \n","            x = Activation('relu')(x)  \n"," \n","            shortcut = x          \n","    return x\n"," \n","def conv3_layer(x):        \n","    shortcut = x\n","    \n","    for i in range(4):     \n","        if(i == 0):            \n","            x = Conv2D(128, (1, 1), strides=(2, 2), padding='valid')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)        \n","            \n","            x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)  \n"," \n","            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\n","            shortcut = Conv2D(512, (1, 1), strides=(2, 2), padding='valid')(shortcut)\n","            x = BatchNormalization()(x)\n","            shortcut = BatchNormalization()(shortcut)            \n"," \n","            x = Add()([x, shortcut])    \n","            x = Activation('relu')(x)    \n"," \n","            shortcut = x              \n","        \n","        else:\n","            x = Conv2D(128, (1, 1), strides=(1, 1), padding='valid')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n","            \n","            x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n"," \n","            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\n","            x = BatchNormalization()(x)            \n"," \n","            x = Add()([x, shortcut])     \n","            x = Activation('relu')(x)\n"," \n","            shortcut = x      \n","            \n","    return x\n"," \n"," \n"," \n","def conv4_layer(x):\n","    shortcut = x        \n","  \n","    for i in range(6):     \n","        if(i == 0):            \n","            x = Conv2D(256, (1, 1), strides=(2, 2), padding='valid')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)        \n","            \n","            x = Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)  \n"," \n","            x = Conv2D(1024, (1, 1), strides=(1, 1), padding='valid')(x)\n","            shortcut = Conv2D(1024, (1, 1), strides=(2, 2), padding='valid')(shortcut)\n","            x = BatchNormalization()(x)\n","            shortcut = BatchNormalization()(shortcut)\n"," \n","            x = Add()([x, shortcut]) \n","            x = Activation('relu')(x)\n"," \n","            shortcut = x               \n","        \n","        else:\n","            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n","            \n","            x = Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n"," \n","            x = Conv2D(1024, (1, 1), strides=(1, 1), padding='valid')(x)\n","            x = BatchNormalization()(x)            \n"," \n","            x = Add()([x, shortcut])    \n","            x = Activation('relu')(x)\n"," \n","            shortcut = x      \n","    return x\n"," \n","def conv5_layer(x):\n","    shortcut = x    \n","  \n","    for i in range(3):     \n","        if(i == 0):            \n","            x = Conv2D(512, (1, 1), strides=(2, 2), padding='valid')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)        \n","            \n","            x = Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)  \n"," \n","            x = Conv2D(2048, (1, 1), strides=(1, 1), padding='valid')(x)\n","            shortcut = Conv2D(2048, (1, 1), strides=(2, 2), padding='valid')(shortcut)\n","            x = BatchNormalization()(x)\n","            shortcut = BatchNormalization()(shortcut)            \n"," \n","            x = Add()([x, shortcut])  \n","            x = Activation('relu')(x)      \n"," \n","            shortcut = x               \n","        \n","        else:\n","            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n","            \n","            x = Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n","            x = BatchNormalization()(x)\n","            x = Activation('relu')(x)\n"," \n","            x = Conv2D(2048, (1, 1), strides=(1, 1), padding='valid')(x)\n","            x = BatchNormalization()(x)           \n","            \n","            x = Add()([x, shortcut]) \n","            x = Activation('relu')(x)       \n"," \n","            shortcut = x                  \n"," \n","    return x\n"," \n","x = conv1_layer(input_tensor)\n","x = conv2_layer(x)\n","x = conv3_layer(x)\n","x = conv4_layer(x)\n","#x = conv5_layer(x)\n"," \n","x = GlobalAveragePooling2D()(x)\n","output_tensor = Dense(K, activation='softmax')(x)\n"," \n","model = Model(input_tensor, output_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6V5ShCOO2dZH","colab_type":"code","colab":{}},"source":["# 모델 컴파일\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xyWoRhy22fE8","colab_type":"code","colab":{}},"source":["# 모델 피팅\n","model.fit(X_train, Y_train, epochs = 100, validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFgjuBEjardO","colab_type":"code","colab":{}},"source":["final_model = models.load_model('/content/models/Resnet40.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bx97m83S2gS2","colab_type":"code","colab":{}},"source":["test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=1)\n","print(test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DmlsuYzF2iah","colab_type":"text"},"source":["## **3. 모델 테스트**"]},{"cell_type":"code","metadata":{"id":"VnxGcNXd3JB4","colab_type":"code","colab":{}},"source":["label_name = ['분노', '혐오', '공포', '기쁨', '중립', '슬픔', '놀람']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9EuadWr3JzV","colab_type":"code","colab":{}},"source":["# 테스트 이미지 파일 디렉토리 연결\n","testing_images = '/content/test_data'\n","\n","# 이미지 파일, 이모션 라벨 리스트\n","test_list = []\n","\n","# 테스트 이미지 리스트에 담기\n","for i in os.listdir(testing_images):\n","    if 'png' or 'tiff' or 'jpg' in i:\n","        test_list.append(testing_images + '/' + i)\n","    else:\n","        pass\n","\n","test_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vWpLd_Hs3Kwx","colab_type":"code","colab":{}},"source":["# 빈 리스트 생성\n","test_result = []\n","test_face = []\n","\n","# 얼굴 디렉터 모듈 초기화\n","detector = dlib.get_frontal_face_detector()\n","# object 디텍션 모듈 초기화\n","objs = dlib.full_object_detections()\n","# 얼굴 특징점 모듈 초기화\n","predictor = dlib.shape_predictor('/content/models/shape_predictor_5_face_landmarks.dat')\n","\n","# align_faces 함수 정의\n","def align_faces(img):\n","    dets = detector(img, 1)\n","    objs = dlib.full_object_detections()\n","    for detection in dets:\n","        s = predictor(img, detection)\n","        objs.append(s)\n","    faces = dlib.get_face_chips(img, objs, size=256, padding=0.35)\n","    return faces\n","\n","for i, img in enumerate(test_list[1:3]):\n","    # img를 image 형태로 변경\n","    img = cv2.imread(img)\n","    h, w = img.shape[:2]\n","    # 얼굴 인식\n","    faces = detector(img)\n","    # 얼굴이 인식되지 않을 경우\n","    if len(faces) == 0:\n","        pass\n","    \n","    # 얼굴이 인식될 경우\n","    else:\n","        # 얼굴이 인식된 원본 사진 저장\n","        test_result.append(img)\n","        # 얼굴이 인식된 얼굴 사진 저장\n","        test_face.append(align_faces(img))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDOdd8nT3LzT","colab_type":"code","colab":{}},"source":["# 이미지 형태 확인\n","check_test = np.array(test_face)\n","check_test = check_test[:, 0, :, :]\n","check_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2nVImlSc3MnA","colab_type":"code","colab":{}},"source":["# test 이미지의 예측 결과\n","preds = model.predict(check_test)\n","#print(preds)\n","\n","# 예측 결과 출력\n","for i, j in enumerate(preds):\n","    # 사진 출력\n","    img = test_result[i]\n","    cv2_imshow(cv2.resize(img, (256, 256), interpolation=True))\n","    # 결과물 출력\n","    print(\"사진의 표정은 {:.4f}%로 {}입니다\".format(max(j)*100,label_name[np.argmax(j)]))"],"execution_count":null,"outputs":[]}]}